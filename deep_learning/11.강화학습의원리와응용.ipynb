{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "81ad1945",
   "metadata": {},
   "source": [
    "# 다중 슬롯머신 문제 (multi armed bandit)\n",
    "\n",
    "- 환경 \n",
    "    - 여러 손잡이 중 하나를 선택해 1달러롤 넣고 당김.\n",
    "        - 그 결과 플레이어는 1달러를 잃거나 1달러를 따게 됨. \n",
    "    - 손잡이마다 승리할 확률이 정해져 있음. \n",
    "        - 돈을 따려면 확률이 높은 손잡이를 당겨야 함.\n",
    "    - 확률은 숨겨져 있음. \n",
    "    \n",
    "- 플레이어가 취할 수 있는 행동 \n",
    "    - 행동에 따라 보상이 주어짐.\n",
    "    - 행동 : 손잡이를 고르는 일 \n",
    "    - 손잡이가 5개라면 행동의 집합은 0, 1, 2, 3, 4\n",
    "    \n",
    "- 보상 \n",
    "    - 돈을 잃거나 따는 것 \n",
    "        - 1달러를 따거나 1달러를 잃거나 \n",
    "    - 보상의 집합은 1, -1 \n",
    "    \n",
    "- 슬롯머신 문제의 특성 \n",
    "    - 다중 슬롯머신 문제에는 상태가 없음. \n",
    "    - 손잡이를 당긴 후에도 환경은 그대로임. \n",
    "        - 손잡이도 그대로, 손잡이마다 설정된 확률도 그대로 \n",
    "    - 다중 슬롯머신은 행동과 보상만 있는 단순한 문제임. \n",
    "        - 강화학습의 기초개념을 익히기에 적합"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d96ce23",
   "metadata": {},
   "source": [
    "# 탐험형 정책과 탐사형 정책\n",
    "\n",
    "- 탐험형 정책(exploration)\n",
    "    - 처음부터 끝까지 손잡이를 무작위로 선택하는 랜덤 정책(random policy)\n",
    "    - 이전 경험을 전혀 사용하지 않는 매우 비효율적인 방식 \n",
    "    \n",
    "- 탐사형 정책(exploitation)\n",
    "    - 몇 번 시도해보고 이후에는 그때까지 가장 높은 확률을 보인 손잡이만 계속 당기는 정책 \n",
    "    - 확률이 더 높은 손잡이를 놓칠 위험이 있음. \n",
    "    \n",
    "- 위의 두 정책은 양 극단에 속함. \n",
    "- 강화학습에서는 둘 사이에서 균형을 잡는 것이 중요\n",
    "     - 예) 현재까지 높은 확률을 보인 손잡이를 더 자주 당기지만 일정 비율로 다른 손잡이도 시도해보는 등\n",
    "     - 이때 다른 손잡이를 시도하는 비율이 높을수록 탐험형에 가깝고 낮을수록 탐사형에 가까움. \n",
    "     \n",
    " - 슬롯머신 문제는 단순하기 때문에 연속으로 행동을 취하고 마칠 때까지의 기록(에피소드)을 충분히 길게 하여 최적의 정책을 찾을 수 있음. \n",
    "     - 랜덤 정책을 길게 적용하여 충분히 긴 에피소드를 수집하고 손잡이마다 돈을 따게 될 확률을 계산\n",
    "         - 예) 1000달러를 가지고 랜덤 정책을 이용해 길이가 1000인 에피소드를 수집\n",
    "         - 모든 손잡이에 대해 확률을 계산하여 네 번째 손잡이의 확률이 가장 높다면 수억을 최대로 보장하는 최적 정책(optimal policy)는 (0,0,0,1,0)\n",
    "         - 네 번째 손잡이를 당기는 확률을 1로 설정하면 많은 돈을 벌 수 있음. \n",
    "     - 하지만 실제로는 확률이 바뀔 수도 있고, 신뢰할만한 확률을 알아낼 수 있을 정도로 돈과 시간이 충분하지 않을 수 있기 때문에 더 영리한 알고리즘 필요 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f7c7a06",
   "metadata": {},
   "source": [
    "# 랜덤 정책을 쓰는 알고리즘 \n",
    "\n",
    "- 알고리즘의 목표는 확률을 모른 채 플레이하면서 수익을 최대화하는 정책을 찾는 것 \n",
    "- 알고리즘에 주어지는 정보는 행동을 했을 때의 보상 뿐 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dbb81103",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 다중 손잡이 밴딧 문제 설정 \n",
    "arms_profit = [0.4, 0.12, 0.52, 0.6, 0.25]\n",
    "n_arms = len(arms_profit)\n",
    "\n",
    "n_trial = 10000 # 손잡이를 당기는 횟수 (에피소드 길이)\n",
    "\n",
    "# 손잡이를 당기는 행위를 시뮬레이션 하는 함수 (handle은 손잡이 번호)\n",
    "def pull_bandit(handle):\n",
    "    q = np.random.random()\n",
    "    if q < arms_profit[handle]:\n",
    "        return 1  # 성공 \n",
    "    else:\n",
    "        return -1  # 실패\n",
    "    \n",
    "# 랜덤 정책을 모방하는 함수 \n",
    "def random_exploration():\n",
    "    episode = []\n",
    "    num = np.zeros(n_arms) # 손잡이별로 당긴 횟수 \n",
    "    wins = np.zeros(n_arms) # 손잡이별로 승리 횟수 \n",
    "    \n",
    "    for i in range(n_trial):\n",
    "        h = np.random.randint(0, n_arms) # 몇 번 핸들 당길건지\n",
    "        reward = pull_bandit(h)\n",
    "        episode.append([h, reward])\n",
    "        num[h] += 1\n",
    "        wins[h] += 1 if reward == 1 else 0\n",
    "        \n",
    "    return episode, (num, wins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d30f3460",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "손잡이별 승리 확률 :  ['0.3967', '0.1363', '0.5360', '0.5950', '0.2681']\n",
      "손잡이별 수익($) :  ['-409', '-1547', '139', '369', '-936']\n",
      "순 수익($) :  -2384\n"
     ]
    }
   ],
   "source": [
    "e, r = random_exploration()\n",
    "\n",
    "print(\"손잡이별 승리 확률 : \", [\"%6.4f\" % (r[1][i]/r[0][i]) if r[0][i] > 0 else 0.0 for i in range(n_arms)])\n",
    "print(\"손잡이별 수익($) : \", [\"%d\" % (2*r[1][i] - r[0][i]) for i in range(n_arms)])\n",
    "print(\"순 수익($) : \", sum(np.asarray(e)[:, 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a6dadfc",
   "metadata": {},
   "source": [
    "# ε-탐욕 알고리즘 (greedy algorithm)\n",
    "\n",
    "- 탐욕 알고리즘 : 과거와 미래를 전혀 고려하지 않고 현재 순간의 정보만으로 선택을 하는 알고리즘 방법론 \n",
    "    - 탐험형과 탐사형 중 탐사형에 치우친 방법론 \n",
    "    \n",
    "- ε-탐욕 알고리즘 \n",
    "    - 기본적으로는 탐욕 알고리즘 \n",
    "    - ε 비율 만큼 탐험을 적용하여 탐험과 탐사의 균형 추구\n",
    "    - 현재까지 파악한 승리 확률에 따라 행동을 선택하는 탐사형 방식을 사용하지만, ε 비율만큼 랜덤 정책을 적용하여 탐험형을 섞음. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e8ecce4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ε-탐욕을 구현하는 함수 \n",
    "def epsilon_greedy(eps):\n",
    "    episode = []\n",
    "    num = np.zeros(n_arms)\n",
    "    wins = np.zeros(n_arms)\n",
    "    \n",
    "    for i in range(n_trial):\n",
    "        r = np.random.random()\n",
    "        \n",
    "        if (r<eps or sum(wins)==0): # 확률 eps 로 임의 선택 \n",
    "            h = np.random.randint(0, n_arms)\n",
    "        else:\n",
    "            prob = np.asarray([wins[i] / num[i] if num[i] > 0 else 0.0 for i in range(n_arms)])\n",
    "            prob = prob / sum(prob)\n",
    "            b = np.random.choice(range(n_arms), p = prob)\n",
    "        \n",
    "        reward = pull_bandit(h)\n",
    "        episode.append([h, reward])\n",
    "        num[h] += 1\n",
    "        wins[h] += 1 if reward == 1 else 0\n",
    "        \n",
    "    return episode, (num, wins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2860b026",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "손잡이별 승리 확률 :  ['0.4023', '0.1157', '0.5211', '0.5867', '0.2543']\n",
      "손잡이별 수익($) :  ['-364', '-1595', '91', '343', '-947']\n",
      "순 수익($) :  -2472\n"
     ]
    }
   ],
   "source": [
    "e, r = epsilon_greedy(0.2)\n",
    "\n",
    "print(\"손잡이별 승리 확률 : \", [\"%6.4f\" % (r[1][i]/r[0][i]) if r[0][i] > 0 else 0.0 for i in range(n_arms)])\n",
    "print(\"손잡이별 수익($) : \", [\"%d\" % (2*r[1][i] - r[0][i]) for i in range(n_arms)])\n",
    "print(\"순 수익($) : \", sum(np.asarray(e)[:, 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85df43e3",
   "metadata": {},
   "source": [
    "- 탐험과 탐사의 균형을 추구하는 ε-탐욕 알고리즘은 이전 정보를 완전히 무시하는 랜덤 정책에 비해 훨씬 영리함. \n",
    "- 현실 세계의 현상 또는 수학적 현상을 난수를 생성하여 시뮬레이션 하는 기법을 통틀어 \"몬테카를로 방법(Monte Carlo method)\"이라고 함. \n",
    "    - 인공지능은 강화학습 뿐만 아니라 다양한 목적으로 몬테카를로 방법을 활용 \n",
    "    - 랜덤 정책 알고리즘과 엡실론-탐욕 알고리즘을 사용한 다중 슬롯머신 문제는 다중 슬롯머신의 동작을 난수로 시뮬레이션했기 때문에 몬테카를로 방법에 속함. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d042a32d",
   "metadata": {},
   "source": [
    "# OpenAI의 gym 라이브러리 \n",
    "\n",
    "- 강화학습을 구현할 때는 OpenAI 재단이 제공하는 gym 라이브러리를 주로 사용 \n",
    "    - 누구나 혜택을 받는 안전한 인공지능을 구현하려는 목표로 창립한 비영리 재단 \n",
    "- gym 라이브러리 \n",
    "    - 강화학습으로 풀어야 하는 문제를 여러가지 제공 \n",
    "    - FrozenLake\n",
    "        - 시작점에서 출발해 한 칸씩 움직여 목표 지점에 도달하면 이기는 게임 \n",
    "        - 중간에 구멍에 빠지면 지고, 밟고 지나갈 수 있는 곳은 정해져 있음. \n",
    "        - 에이전트에는 어떤 칸이 구멍이고 어떤 칸이 지나갈 수 있는지에 대한 정보는 주어지지 않음. \n",
    "    \n",
    "    - CartPole\n",
    "        - 막대를 오래 세워놓을수록 높은 점수를 받는 문제 \n",
    "        - 막대가 왼쪽으로 기울어지면 수레를 왼쪽으로 움직여 평형을 이뤄야함. \n",
    "        - 너무 극단적으로 많이 이동하면 오히려 회복이 불가능해짐. \n",
    "        \n",
    "    - MountainCar \n",
    "        - 언덕에 있는 깃발에 도달하면 이기는 게임 \n",
    "        - 반대쪽 언덕으로 충분히 올라간 다음 구르는 힘으로 목적지에 도달해야함. \n",
    "        \n",
    "    - FetchSlide\n",
    "        - 로봇이 손으로 검은색 퍽을 쳐서 특정 지점으로 옮기는 문제\n",
    "        - 정확한 방향을 향해 적절한 힘을 가해야 성공할 수 있음.\n",
    "        \n",
    "    - Atari\n",
    "        - 1980년대에 유행하던 고전 비디오게임 수십종이 제공됨 \n",
    "        - 대표적인 것은 Breakout, BankHeist, FishingDerby"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c15367e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting swig\n",
      "  Using cached swig-4.1.1-py2.py3-none-win_amd64.whl (2.5 MB)\n",
      "Installing collected packages: swig\n",
      "Successfully installed swig-4.1.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -entencepiece (c:\\users\\answl\\anaconda3\\envs\\nlp\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -entencepiece (c:\\users\\answl\\anaconda3\\envs\\nlp\\lib\\site-packages)\n",
      "\n",
      "[notice] A new release of pip is available: 23.1.2 -> 23.2.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install swig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b80ea57a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gym[all]\n",
      "  Using cached gym-0.26.2-py3-none-any.whl\n",
      "Requirement already satisfied: numpy>=1.18.0 in c:\\users\\answl\\appdata\\roaming\\python\\python37\\site-packages (from gym[all]) (1.21.6)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in c:\\users\\answl\\anaconda3\\envs\\nlp\\lib\\site-packages (from gym[all]) (2.0.0)\n",
      "Collecting gym-notices>=0.0.4 (from gym[all])\n",
      "  Using cached gym_notices-0.0.8-py3-none-any.whl (3.0 kB)\n",
      "Requirement already satisfied: importlib-metadata>=4.8.0 in c:\\users\\answl\\appdata\\roaming\\python\\python37\\site-packages (from gym[all]) (6.6.0)\n",
      "Requirement already satisfied: swig==4.* in c:\\users\\answl\\anaconda3\\envs\\nlp\\lib\\site-packages (from gym[all]) (4.1.1)\n",
      "Collecting mujoco==2.2 (from gym[all])\n",
      "  Using cached mujoco-2.2.0-cp37-cp37m-win_amd64.whl (2.7 MB)\n",
      "Collecting opencv-python>=3.0 (from gym[all])\n",
      "  Using cached opencv_python-4.8.0.76-cp37-abi3-win_amd64.whl (38.1 MB)\n",
      "Collecting moviepy>=1.0.0 (from gym[all])\n",
      "  Using cached moviepy-1.0.3-py3-none-any.whl\n",
      "Collecting pytest==7.0.1 (from gym[all])\n",
      "  Using cached pytest-7.0.1-py3-none-any.whl (296 kB)\n",
      "Collecting box2d-py==2.3.5 (from gym[all])\n",
      "  Using cached box2d-py-2.3.5.tar.gz (374 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting lz4>=3.1.0 (from gym[all])\n",
      "  Using cached lz4-4.3.2-cp37-cp37m-win_amd64.whl (99 kB)\n",
      "Requirement already satisfied: imageio>=2.14.1 in c:\\users\\answl\\anaconda3\\envs\\nlp\\lib\\site-packages (from gym[all]) (2.19.3)\n",
      "Collecting pygame==2.1.0 (from gym[all])\n",
      "  Using cached pygame-2.1.0-cp37-cp37m-win_amd64.whl (4.8 MB)\n",
      "Collecting mujoco-py<2.2,>=2.1 (from gym[all])\n",
      "  Using cached mujoco_py-2.1.2.14-py3-none-any.whl (2.4 MB)\n",
      "Requirement already satisfied: matplotlib>=3.0 in c:\\users\\answl\\anaconda3\\envs\\nlp\\lib\\site-packages (from gym[all]) (3.5.2)\n",
      "Collecting ale-py~=0.8.0 (from gym[all])\n",
      "  Using cached ale_py-0.8.1-cp37-cp37m-win_amd64.whl (952 kB)\n",
      "Requirement already satisfied: absl-py in c:\\users\\answl\\appdata\\roaming\\python\\python37\\site-packages (from mujoco==2.2->gym[all]) (1.4.0)\n",
      "Collecting glfw (from mujoco==2.2->gym[all])\n",
      "  Using cached glfw-2.6.2-py2.py27.py3.py30.py31.py32.py33.py34.py35.py36.py37.py38-none-win_amd64.whl (493 kB)\n",
      "Collecting pyopengl (from mujoco==2.2->gym[all])\n",
      "  Using cached PyOpenGL-3.1.7-py3-none-any.whl (2.4 MB)\n",
      "Requirement already satisfied: attrs>=19.2.0 in c:\\users\\answl\\anaconda3\\envs\\nlp\\lib\\site-packages (from pytest==7.0.1->gym[all]) (21.4.0)\n",
      "Requirement already satisfied: iniconfig in c:\\users\\answl\\anaconda3\\envs\\nlp\\lib\\site-packages (from pytest==7.0.1->gym[all]) (1.1.1)\n",
      "Requirement already satisfied: packaging in c:\\users\\answl\\appdata\\roaming\\python\\python37\\site-packages (from pytest==7.0.1->gym[all]) (23.1)\n",
      "Requirement already satisfied: pluggy<2.0,>=0.12 in c:\\users\\answl\\anaconda3\\envs\\nlp\\lib\\site-packages (from pytest==7.0.1->gym[all]) (1.0.0)\n",
      "Requirement already satisfied: py>=1.8.2 in c:\\users\\answl\\anaconda3\\envs\\nlp\\lib\\site-packages (from pytest==7.0.1->gym[all]) (1.11.0)\n",
      "Requirement already satisfied: tomli>=1.0.0 in c:\\users\\answl\\anaconda3\\envs\\nlp\\lib\\site-packages (from pytest==7.0.1->gym[all]) (2.0.1)\n",
      "Requirement already satisfied: atomicwrites>=1.0 in c:\\users\\answl\\anaconda3\\envs\\nlp\\lib\\site-packages (from pytest==7.0.1->gym[all]) (1.4.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\answl\\anaconda3\\envs\\nlp\\lib\\site-packages (from pytest==7.0.1->gym[all]) (0.4.6)\n",
      "Requirement already satisfied: importlib-resources in c:\\users\\answl\\anaconda3\\envs\\nlp\\lib\\site-packages (from ale-py~=0.8.0->gym[all]) (5.2.0)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\answl\\anaconda3\\envs\\nlp\\lib\\site-packages (from ale-py~=0.8.0->gym[all]) (4.4.0)\n",
      "Requirement already satisfied: pillow>=8.3.2 in c:\\users\\answl\\anaconda3\\envs\\nlp\\lib\\site-packages (from imageio>=2.14.1->gym[all]) (9.2.0)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\answl\\appdata\\roaming\\python\\python37\\site-packages (from importlib-metadata>=4.8.0->gym[all]) (3.15.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\answl\\anaconda3\\envs\\nlp\\lib\\site-packages (from matplotlib>=3.0->gym[all]) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\answl\\anaconda3\\envs\\nlp\\lib\\site-packages (from matplotlib>=3.0->gym[all]) (4.25.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\answl\\anaconda3\\envs\\nlp\\lib\\site-packages (from matplotlib>=3.0->gym[all]) (1.4.2)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in c:\\users\\answl\\anaconda3\\envs\\nlp\\lib\\site-packages (from matplotlib>=3.0->gym[all]) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\answl\\anaconda3\\envs\\nlp\\lib\\site-packages (from matplotlib>=3.0->gym[all]) (2.8.2)\n",
      "Collecting decorator<5.0,>=4.0.2 (from moviepy>=1.0.0->gym[all])\n",
      "  Using cached decorator-4.4.2-py2.py3-none-any.whl (9.2 kB)\n",
      "Requirement already satisfied: tqdm<5.0,>=4.11.2 in c:\\users\\answl\\anaconda3\\envs\\nlp\\lib\\site-packages (from moviepy>=1.0.0->gym[all]) (4.64.1)\n",
      "Requirement already satisfied: requests<3.0,>=2.8.1 in c:\\users\\answl\\appdata\\roaming\\python\\python37\\site-packages (from moviepy>=1.0.0->gym[all]) (2.29.0)\n",
      "Collecting proglog<=1.0.0 (from moviepy>=1.0.0->gym[all])\n",
      "  Using cached proglog-0.1.10-py3-none-any.whl (6.1 kB)\n",
      "Collecting imageio-ffmpeg>=0.2.0 (from moviepy>=1.0.0->gym[all])\n",
      "  Using cached imageio_ffmpeg-0.4.8-py3-none-win_amd64.whl (22.6 MB)\n",
      "Requirement already satisfied: Cython>=0.27.2 in c:\\users\\answl\\anaconda3\\envs\\nlp\\lib\\site-packages (from mujoco-py<2.2,>=2.1->gym[all]) (0.29.32)\n",
      "Requirement already satisfied: cffi>=1.10 in c:\\users\\answl\\anaconda3\\envs\\nlp\\lib\\site-packages (from mujoco-py<2.2,>=2.1->gym[all]) (1.15.1)\n",
      "Collecting fasteners~=0.15 (from mujoco-py<2.2,>=2.1->gym[all])\n",
      "  Using cached fasteners-0.18-py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: pycparser in c:\\users\\answl\\anaconda3\\envs\\nlp\\lib\\site-packages (from cffi>=1.10->mujoco-py<2.2,>=2.1->gym[all]) (2.21)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\answl\\appdata\\roaming\\python\\python37\\site-packages (from python-dateutil>=2.7->matplotlib>=3.0->gym[all]) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\answl\\anaconda3\\envs\\nlp\\lib\\site-packages (from requests<3.0,>=2.8.1->moviepy>=1.0.0->gym[all]) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\answl\\anaconda3\\envs\\nlp\\lib\\site-packages (from requests<3.0,>=2.8.1->moviepy>=1.0.0->gym[all]) (2.6)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\answl\\anaconda3\\envs\\nlp\\lib\\site-packages (from requests<3.0,>=2.8.1->moviepy>=1.0.0->gym[all]) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\answl\\appdata\\roaming\\python\\python37\\site-packages (from requests<3.0,>=2.8.1->moviepy>=1.0.0->gym[all]) (2022.12.7)\n",
      "Building wheels for collected packages: box2d-py\n",
      "  Building wheel for box2d-py (setup.py): started\n",
      "  Building wheel for box2d-py (setup.py): finished with status 'done'\n",
      "  Created wheel for box2d-py: filename=box2d_py-2.3.5-cp37-cp37m-win_amd64.whl size=432677 sha256=7b67a9f49653ddb4690648960a451a1dc2ccd73ef9ecd1d60cf7d4053307de69\n",
      "  Stored in directory: c:\\users\\answl\\appdata\\local\\pip\\cache\\wheels\\55\\c5\\e2\\d4b1b4deb6fa3977c98d1e402eefe456a299f2ba6eb4c44d82\n",
      "Successfully built box2d-py\n",
      "Installing collected packages: pyopengl, gym-notices, glfw, box2d-py, pygame, opencv-python, mujoco, lz4, imageio-ffmpeg, fasteners, decorator, proglog, mujoco-py, gym, ale-py, pytest, moviepy\n",
      "  Attempting uninstall: decorator\n",
      "    Found existing installation: decorator 5.1.1\n",
      "    Uninstalling decorator-5.1.1:\n",
      "      Successfully uninstalled decorator-5.1.1\n",
      "  Attempting uninstall: pytest\n",
      "    Found existing installation: pytest 7.1.2\n",
      "    Uninstalling pytest-7.1.2:\n",
      "      Successfully uninstalled pytest-7.1.2\n",
      "Successfully installed ale-py-0.8.1 box2d-py-2.3.5 decorator-4.4.2 fasteners-0.18 glfw-2.6.2 gym-0.26.2 gym-notices-0.0.8 imageio-ffmpeg-0.4.8 lz4-4.3.2 moviepy-1.0.3 mujoco-2.2.0 mujoco-py-2.1.2.14 opencv-python-4.8.0.76 proglog-0.1.10 pygame-2.1.0 pyopengl-3.1.7 pytest-7.0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -entencepiece (c:\\users\\answl\\anaconda3\\envs\\nlp\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -entencepiece (c:\\users\\answl\\anaconda3\\envs\\nlp\\lib\\site-packages)\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "spyder 5.2.2 requires pyqt5<5.13, which is not installed.\n",
      "spyder 5.2.2 requires pyqtwebengine<5.13, which is not installed.\n",
      "\n",
      "[notice] A new release of pip is available: 23.1.2 -> 23.2.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install gym[all]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e9a25443",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting opencv-python==4.5.5.64\n",
      "  Downloading opencv_python-4.5.5.64-cp36-abi3-win_amd64.whl (35.4 MB)\n",
      "     ---------------------------------------- 35.4/35.4 MB 1.6 MB/s eta 0:00:00\n",
      "Requirement already satisfied: numpy>=1.14.5 in c:\\users\\answl\\appdata\\roaming\\python\\python37\\site-packages (from opencv-python==4.5.5.64) (1.21.6)\n",
      "Installing collected packages: opencv-python\n",
      "  Attempting uninstall: opencv-python\n",
      "    Found existing installation: opencv-python 4.8.0.76\n",
      "    Uninstalling opencv-python-4.8.0.76:\n",
      "      Successfully uninstalled opencv-python-4.8.0.76\n",
      "Successfully installed opencv-python-4.5.5.64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -entencepiece (c:\\users\\answl\\anaconda3\\envs\\nlp\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -entencepiece (c:\\users\\answl\\anaconda3\\envs\\nlp\\lib\\site-packages)\n",
      "\n",
      "[notice] A new release of pip is available: 23.1.2 -> 23.2.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install opencv-python==4.5.5.64"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d35c608a",
   "metadata": {},
   "source": [
    "# FrozenLake 문제 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e0f59df5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discrete(16)\n",
      "Discrete(4)\n",
      "[[1, 0.0, 4], [1, 0.0, 8], [1, 0.0, 12], [2, 0.0, 1], [3, 0.0, 1], [3, 0.0, 1], [2, 0.0, 2], [2, 0.0, 3], [2, 0.0, 3], [2, 0.0, 3], [1, 0.0, 7], [2, 0.0, 1], [3, 0.0, 1], [2, 0.0, 2], [1, 0.0, 6], [0, 0.0, 5], [3, 0.0, 0], [3, 0.0, 0], [2, 0.0, 1], [0, 0.0, 0]]\n"
     ]
    }
   ],
   "source": [
    "import gym \n",
    "\n",
    "# 환경 불러오기 \n",
    "env = gym.make('FrozenLake-v1', is_slippery=False, render_mode='human')\n",
    "print(env.observation_space)\n",
    "print(env.action_space)\n",
    "\n",
    "n_trial = 20\n",
    "\n",
    "# 에피소드 수집 \n",
    "env.reset()\n",
    "episode = []\n",
    "\n",
    "for i in range(n_trial):\n",
    "    action = env.action_space.sample()\n",
    "    obs, reward, terminated, truncated, info = env.step(action)\n",
    "    episode.append([action, reward, obs])\n",
    "    env.render()\n",
    "    \n",
    "    if terminated or truncated:\n",
    "        observation, info = env.reset()\n",
    "        \n",
    "print(episode)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "243b119c",
   "metadata": {},
   "source": [
    "- Discrete(16)(observation_space)\n",
    "    - 16개의 상태가 있다는 뜻 \n",
    "    - 4x4 격자 위의 어느 곳에 있는지를 표현 \n",
    "    - FrozenLake 환경 \n",
    "        - SFFF\n",
    "        - FHFH\n",
    "        - FFFH\n",
    "        - HFFG\n",
    "        \n",
    "- Discrete(4)(action_space)\n",
    "    - 상하좌우로 이동하는 네 가지 행동이 있다는 뜻 \n",
    "        - 0 : Left\n",
    "        - 1 : Down\n",
    "        - 2 : Right\n",
    "        - 3 : Up \n",
    "        \n",
    "- episode \n",
    "    - 예) [1, 0.0, 4] : 행동 1을 하고 보상 0.0을 받고 상태 4로 전환 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d5d190d",
   "metadata": {},
   "source": [
    "# 계산 모형 \n",
    "\n",
    "- 강화학습이 풀어야 하는 문제는 환경(environment)으로 정의됨. \n",
    "- 환경 \n",
    "    - 상태의 종류, 행동의 종류, 보상의 종류를 지정하며 행동을 취했을 때 발생하는 상태 변환을 지배하는 규칙을 포함하는 개념 \n",
    "    - 이런 정보를 통틀어 \"마르코프 결정 프로세스(MDP: Markov Decision Process)\"라고 함.\n",
    "    \n",
    "- 상태와 행동, 보상 \n",
    "    - 에이전트와 환경은 밀접하게 상호작용하며, 에이전트는 환경이 제공하는 상태와 보상에 따라 행동을 취함. \n",
    "    - 슬롯머신 문제는 기계 앞에서 도박을 하는 사람, FrozenLake 문제에서는 호수를 건너는 사람이 에이전트가 됨. \n",
    "    \n",
    "- 슬롯머신 문제와 FrozenLake 문제를 MDP에 대입하면\n",
    "    - 슬롯머신 문제\n",
    "        - 상태 집합 : 공집합 \n",
    "        - 행동 집합 : {손잡이0, 손잡이1, 손잡이2, 손잡이3, 손잡이4}\n",
    "        - 보상 집합 : {1, -1} (즉시 보상)\n",
    "        \n",
    "    - FrozenLake 문제 \n",
    "        - 상태 집합 : {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15}\n",
    "        - 행동 집합 : {0(left), 1(down), 2(right), 3(up)}\n",
    "        - 보상 집합 : {0, 1} (지연된 보상)\n",
    "        \n",
    "- 보상이 주어지는 시점 \n",
    "    - 슬롯머신 문제에서는 손잡이를 당겨 행동을 취하면 즉시 보상이 주어짐. \n",
    "    - FrozenLake 에서는 목표 지점에 도달하면 1이라는 보상이 주어지는데, 그 외에는 0이 주어짐. \n",
    "        - 중간에 0이라는 보상이 주어지는 것은 보상 총액에 영향을 미치지 않기 때문에 의미가 없음. \n",
    "        - FrozenLake와 같은 경우를 \"지연된 보상(delayed reward)\"라고 함. \n",
    "        - 바둑과 장기도 지연된 보상 체계 \n",
    "        \n",
    "- 보상 전이 \n",
    "    - 어떤 상태에서 행동을 취하면 새로운 상태로 전이 \n",
    "        - 예) FrozenLake 문제에서 1의 칸에서 2(Right)라는 행동을 취한다면 \n",
    "            - 주어지는 보상 : 0\n",
    "            - 새로운 상태 : 2라는 칸으로 이동하기 때문에 2\n",
    "                - 이때 새로운 상태는 항상 2임 \n",
    "                - 100% 확률로 새로운 상태가 정해지는 환경을 \"결정론적 환경(deterministic environment)\"이라고 함.\n",
    "                    - 바둑, 장기, 비디오 게임 등은 모두 결정론적 환경에 해당 \n",
    "                - FrozenLake 문제에서 얼음 바닥이 미끄러워 오른쪽 행동을 취했는데, 다른 방향으로 이동하게 되는 상황을 허용한다면 \n",
    "                    - \"스토케스틱 환경(Stochastic Environment)\"이 됨. \n",
    "                    - is_slippery=True로 설정하면 스토케스틱 환경이 됨."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d3a9af",
   "metadata": {},
   "source": [
    "# 최적 정책 \n",
    "\n",
    "- 강화 학습은 누적 보상을 최대화하는 최적 정책을 알아내야 함. \n",
    "- 최적 정책 \n",
    "    - 슬롯머신 문제에서는 손잡이 3의 확률이 가장 높기 때문에 항상 손잡이 3을 당기게 됨. \n",
    "    - 정책은 확률분포로 표현할 수 있음. \n",
    "        - 현재 상태에서 어떤 행동을 취할지를 결정하는 확률 분포 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecc9b1e1",
   "metadata": {},
   "source": [
    "- 『알파제로를 분석하며 배우는 인공지능』"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
