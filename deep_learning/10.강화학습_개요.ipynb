{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4bf19f90",
   "metadata": {},
   "source": [
    "# 강화학습 (Reinforcement Learning)\n",
    "\n",
    "- 20세기 행동심리학에서 \"강화(Reinforcement)\"라는 개념이 등장\n",
    "    - 동물이 시행착오를 통해 학습하는 방법 중 하나\n",
    "        - 동물이 이전에 배우지 않았지만 직접 시도하면서 행동과 그 결과로 나타나는 좋은 보상 사이의 상관관계를 학습하는 것 \n",
    "        - 점차 좋은 보상을 얻게 해주는 행동을 점점 더 많이 하게 됨.\n",
    "            - 보상이 왜 나오는지에 대해 이해하는 것은 아님. \n",
    "            \n",
    "    - 강화는 사람에게도 익숙한 개념임.\n",
    "        - 아기가 처음 걷는 것을 배울 때 걷는 법을 배우지 않아도 스스로 이것저것 시도해보면서 걸음."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69296f51",
   "metadata": {},
   "source": [
    "# 머신러닝에서의 강화학습\n",
    "\n",
    "<img src = \"agent_env.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c1616f5",
   "metadata": {},
   "source": [
    "- 지도학습, 비지도학습과 다르게 정답이 주어지지도 않고 주어진 데이터에 대해 학습하지도 않음. \n",
    "\n",
    "- 강화학습은 보상(reward)을 통해 학습 \n",
    "    - 보상 : 컴퓨터가 선택한 행동에 대한 환경의 반응 \n",
    "    \n",
    "- 에이전트(agent) : 강화학습을 통해 스스로 학습하는 컴퓨터 \n",
    "    - 에이전트는 환경에 대해 사전 지식이 없는 상태에서 학습 \n",
    "    - 자신이 놓인 환경에서 자신의 상태를 인식한 후 행동 \n",
    "    - 환경은 에이전트에게 행동에 대한 보상을 주고 다음 상태를 알려줌\n",
    "    - 우연히 받은 보상을 통해 에이전트는 어떤 행동이 좋은지 간접적으로 알게 됨. \n",
    "        - 강화학습의 목적은 에이전트가 환경을 탐색하면서 얻는 보상들의 합을 최대화하는 \"최적의 행동양식, 또는 정책\"을 학습하는 것 \n",
    "        \n",
    "    - 시행착오를 많이 거쳐야 하기 때문에 학습시간이 굉장히 오래 걸림. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f2585e2",
   "metadata": {},
   "source": [
    "# 강화학습의 장점 \n",
    "\n",
    "- 환경에 대한 사전지식이 없어도 학습 \n",
    "    - 알파고는 바둑이라는 게임의 규칙과 사전지식이 없는 상태에서 학습 시작 \n",
    "    - 처음에는 무작위로 바둑돌을 놓다가 우연히 상대방을 이기게 됨. \n",
    "        - 보상을 받고 상대방을 이기게 한 행동을 더 하려고 함. \n",
    "    - 이기는 횟수가 늘어갈수록 어떤 상황에서 돌을 어디에 놓아야 이기게 되는지를 학습 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fb44681",
   "metadata": {},
   "source": [
    "# 강화학습 문제 \n",
    "\n",
    "- 강화학습은 사람처럼 환경과 상호작용하면서 스스로 학습하는 방식 \n",
    "- 결정을 순차적으로 내려야하는 문제에 주로 적용됨. \n",
    "    - 현재 위치에서 행동을 한 번 선택하는 것이 아니라 지속적으로 선택해야 함. \n",
    "- 일반적으로 순차적으로 결정을 내리는 문제는 다이내믹 프로그래밍 또는 진화 알고리즘을 적용할 수 있지만, 강화학습은 다이내믹 프로그래밍과 진화 알고리즘의 한계를 극복할 수 있음. \n",
    "    - 진화 알고리즘(유전자 알고리즘) : 가장 성능이 좋았던 모델 ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa0dad3",
   "metadata": {},
   "source": [
    "# 순차적 행동 결정 문제 \n",
    "\n",
    "- 순차적 행동 결정 문제를 수학적으로 표현하기 위한 가장 간단한 방법은 문제를 수치화하는 것\n",
    "    - 예) 시험을 통해 수학 능력을 수치화 \n",
    "        - 시험 점수가 학생들의 진정한 수학 실력을 나타내주지 않을 수 있지만 대체적으로 수학 실력과 비례 \n",
    "        - 만약 수학 실력을 수치화하지 않는다면 수학 점수를 높이기 위한 전략을 세우기 어려움. \n",
    "        \n",
    "- 강화학습도 마찬가지로 에이전트가 학습하고 발전하려면 문제를 수학적으로 표현할 수 있어야 함. \n",
    "    - 이때 사용하는 방법이 MDP(Markov Decision Process)\n",
    "        - 순차적 행동 결정 문제를 수학적으로 정의해서 에이전트가 순차적 행동 결정 문제에 접근할 수 있게 함. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bd665cb",
   "metadata": {},
   "source": [
    "# 순차적 행동 결정 문제의 구성 요소\n",
    "\n",
    "- 수학적으로 정의된 문제는 아래의 구성 요소를 가짐. \n",
    "\n",
    "1. 상태 (state)\n",
    "    - 에이전트의 상태 \n",
    "    - 어떠한 \"정적인 요소\"를 포함하며 에이전트가 움직이는 속도와 같은 \"동적인 요소\" 또한 포함하는 표현 \n",
    "        - 에이전트가 상황을 판단해서 행동을 결정하기에 충분한 정보를 제공해야함. \n",
    "        - 예) 탁구를 치는 에이전트라면 탁구공의 위치 뿐만 아니라 탁구공의 속도, 가속도 등의 정보가 필요함. \n",
    "        \n",
    "2. 행동 (action)\n",
    "    - 에이전트가 어떠한 상태에서 취할 수 있는 행동 \n",
    "        - 예) 게임에서의 행동이라면 게임기를 통해 줄 수 있는 입력 \n",
    "    - 학습이 되지 않은 에이전트는 어떤 행동이 좋은 행동인지에 대한 정보가 없어서 무작위로 행동함. \n",
    "    - 하지만 학습하면서 특정한 행동들을 할 확률을 높임. \n",
    "    - 에이전트가 행동을 취하면 환경은 에이전트에게 보상을 주고 다음 상태를 알려줌. \n",
    "    \n",
    "3. 보상 (reward)\n",
    "    - 강화학습을 다른 머신러닝 기법과 다르게 만들어주는 가장 핵심적인 요소 \n",
    "    - 에이전트가 학습할 수 있는 유일한 정보 \n",
    "    - 보상을 통해 에이전트는 자신의 행동을 평가 \n",
    "    - 보상은 에이전트에 속하지 않는 환경의 일부 \n",
    "        - 에이전트는 어떤 상황에서 얼마의 보상이 나오는지 알지 못함. \n",
    "        \n",
    "4. 정책 (policy)\n",
    "    - 순차적 행동 결정 문제에서 구해야 할 답 \n",
    "    - 특정 상태가 아닌 모든 상태에 대해 에이전트가 어떤 행동을 해야 하는지 정해놓은 것 \n",
    "    - 제일 좋은 정책은 \"최적 정책(optimal policy)\"이라 하며 에이전트는 최적 정책에 따라 행동했을 때 보상의 합을 최대로 받을 수 있음. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27f5eb6a",
   "metadata": {},
   "source": [
    "# 강화학습 예시 (브레이크 아웃)\n",
    "\n",
    "<img src = \"breakout.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bc9a11e",
   "metadata": {},
   "source": [
    "- 아타리의 고전 게임 \n",
    "    - 구글 딥마인드의 『Playing Atari with deep Reinforcement Learning』 논문에서 아타리 게임에 강화학습을 적용\n",
    "    \n",
    "<br>\n",
    "\n",
    "- 브레이크아웃의 MDP와 학습 방법 \n",
    "    1. MDP \n",
    "        - 브레이크아웃에서 에이전트가 환경으로부터 받아들이는 상태는 게임 화면 \n",
    "        - 따라서 에이전트가 상황을 파악할 수 있도록 같은 화면을 일정한 시간 간격으로 연속으로 4개를 받아서 하나의 상태로 에이전트에게 제공 \n",
    "            - 공의 속도 파악 가능 --> 이에 따라 행동 결정 \n",
    "            - (초기) 게임 화면은 흑백화면이기 때문에 2차원 픽셀 데이터\n",
    "        - 행동 : 제자리, 왼쪽, 오른쪽, *발사 (발사는 게임 시작 시 사용)\n",
    "            - 사실상 게임 도중에는 제자리, 왼쪽, 오른쪽만 가능 \n",
    "        - 보상 ; 벽돌이 하나씩 깨질 때마다 보상을 +1씩 받고 더 위쪽을 깰수록 더 큰 보상을 받음. \n",
    "            - 아무것도 깨지 않을 때는 보상 0, 공을 놓쳐서 목숨을 잃을 경우에는 보상 -1\n",
    "            \n",
    "    2. 학습 \n",
    "        - 처음에는 에이전트가 게임이나 상황을 모르기 때문에 무작위로 제자리, 오른쪽, 왼쪽으로 움직임.\n",
    "        - 그러다가 우연히 공을 쳐서 벽돌을 깨면 게임(환경)으로부터 +1의 보상을 받고, 공을 놓친다면 -1의 보상을 받음. \n",
    "        - 위 과정의 반복을 통해 어떻게 해야 공을 떨어뜨리지 않고 벽을 깰 수 있는지 학습\n",
    "        - 왼쪽 상단 숫자는 누적되는 보상 \n",
    "            - 에이전트는 게임으로부터 즉각적인 보상을 받지만, 에이전트의 목표는 즉각적인 보상이 아닌 누적되는 보상의 합을 최대화하는 것\n",
    "        - 강화학습을 통해 인공신경망을 학습 \n",
    "            - 인공신경망으로 입력(4개의 연속적인 게임 화면)이 들어오면 그 상태에서 에이전트가 할 수 있는 행동이 얼마나 좋은지를 출력\n",
    "            - 행동이 얼마나 좋은지가 행동의 가치 (점수를 받는 데 얼마나 유리한지)\n",
    "            - 이것을 계산하는 함수를 \"큐 함수(Q function)\"라고 함. \n",
    "                - 해당 논문에서는 \"DQN(Deep Q-Network)\" 인공신경망 사용 \n",
    "                    - DQN으로 상태가 입력으로 들어오면 그 상태에서 제자리, 왼쪽, 오른쪽 행동의 큐함수를 출력으로 내놓음. \n",
    "            - 에이전트는 큐함수에 따라 행동\n",
    "                - 큰 가치를 지니는 행동 선택 \n",
    "            - 에이전트가 행동을 취하면 에이전트에게 보상과 다음 상태 알려줌. \n",
    "            - 에이전트는 환경과 상호작용하면서 DQN을 더 많은 보상을 받도록 조정 (학습 과정)\n",
    "        - 초반에 에이전트는 경험한 것이 적기 때문에 최적이라고 판단한 것이 진짜 최적일지 알 수 없음.\n",
    "            - 학습을 계속하다가 우연히 터널을 뚫게 되면 이를 통해 에이전트는 \"전략\"을 학습 \n",
    "        - 에이전트와 사람의 공통점\n",
    "            - 게임 화면을 보고 학습 \n",
    "        - 에이전트와 사람의 차이점 \n",
    "            - 에이전트는 게임의 규칙을 모름.\n",
    "                - 강화학습의 장점이자 단점 (느린 학습)\n",
    "            - 사람은 하나를 학습하면 다른 곳에서도 활용할 수 있음. (원리를 기억하니까)\n",
    "                - 예) 수학을 잘 배우면 과학을 배우기도 수월함. \n",
    "            - 강화학습은 각 학습을 모두 별개로 취급해서 항상 처음부터 학습해야함. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
