{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "85482959",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c0ac23",
   "metadata": {},
   "source": [
    "# 데이터 전처리\n",
    "\n",
    "- 픽셀값을 0 ~ 1 사이로 스케일링 \n",
    "- 2차원 배열을 1차원 배열로 변환\n",
    "- 훈련 / 검증 나누기 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7c8417cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_train = x_train / 255.0 # 스케일링 \n",
    "scaled_train = scaled_train.reshape(-1, 28*28) # 평탄화\n",
    "\n",
    "scaled_train, scaled_val, y_train, y_val = train_test_split(scaled_train,\n",
    "                                                           y_train,\n",
    "                                                           test_size=0.2,\n",
    "                                                           random_state=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3abcc6e",
   "metadata": {},
   "source": [
    "# 심층신경망 구성 \n",
    "\n",
    "- 인공 신경망에 층을 추가한 구조 \n",
    "\n",
    "<img src = \"ml_perceptron.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87a6229e",
   "metadata": {},
   "source": [
    "- 단층 신경망과의 차이는 입력층과 출력층 사이 밀집층이 추가된 것 \n",
    "    - 입력층과 출력층 사이에 있는 모든 층을 은닉층(hidden layer)라고 부름.\n",
    "\n",
    "- 출력층에 적용하는 활성화 함수와 은닉층에 적용하는 활성화 함수는 차이가 있음.\n",
    "    - 출력층의 활성화 함수 \n",
    "        - \"출력층 함수\"라고도 부름\n",
    "        - 결과물을 적절한 형식으로 출력하도록 유도해서, 데이터셋과 잘 비교할 수 있도록 하는 역할 \n",
    "        - 종류에 제한이 있음 (이진분류:시그모이드, 다중분류:소프트맥스 ...)\n",
    "        - 출력층에는 활성화 함수가 없을 수도 있음 (ex. 회귀 --> 예측값을 그냥 그대로 쓰면 됨)\n",
    "        \n",
    "    - 은닉층의 활성화 함수 \n",
    "        - 여러 겹의 layer들 사이에서 사용됨 \n",
    "        - 출력층 함수에 비해 선택이 자유로움 \n",
    "        - 대표적인 활성화 함수: ReLU(렐루)\n",
    "        - 모든 신경망의 은닉층에는 항상 활성화 함수가 있음. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "410173b4",
   "metadata": {},
   "source": [
    "- 활성화 함수(activation function)\n",
    "    - 활성화 함수를 쓰는 이유 \n",
    "        - 예) a*4 + 2 = b\n",
    "        -     b*3 - 5 = c\n",
    "        - 위 2개 식은 a*12 + 1 = c로 단순화 가능 \n",
    "    - 은닉층이 선형적인 산술계산만 한다면 층이 깊어진다고 하더라도 계산식이 단순화되어 학습 효율이 떨어짐. \n",
    "        - 따라서 활성화 함수로 선형계산을 비선형계산으로 비틀어주는 과정이 필요\n",
    "        \n",
    "<img src = \"activation.jpg\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3f9644f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 은닉층 \n",
    "dense1 = keras.layers.Dense(100, activation='sigmoid', input_shape=(784,))\n",
    "# 출력층 \n",
    "dense2 = keras.layers.Dense(10, activation='softmax')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f1459fd",
   "metadata": {},
   "source": [
    "- dense1 \n",
    "    - 은닉층 \n",
    "    - 100개의 유닛을 가진 밀집층 \n",
    "        - 유닛 개수를 정하는 것은 특별한 기준이 없음.\n",
    "        - 다만 출력층의 유닛보다는 많은 것이 좋음 \n",
    "            - 은닉층의 유닛이 출력층보다 적다면 전달되는 정보량이 부족해질 수 있음.\n",
    "    - 활성화 함수는 시그모이드 (0~1 사이값) --> 다른것도 가능 \n",
    "    - 입력층과 연결되기 때문에 입력의 크기는 (784,)\n",
    "    \n",
    "- dense2 \n",
    "    - 출력층 \n",
    "    - 10개의 클래스로 분류하므로 10개의 유닛 \n",
    "    - 다중 분류이기 때문에 활성화 함수는 소프트맥스"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d6d27315",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "591f61da",
   "metadata": {},
   "source": [
    "- **가장 처음 등장하는 은닉층부터 마지막 출력층까지 순서대로 추가해야함**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4df8c0f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 100)               78500     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                1010      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 79,510\n",
      "Trainable params: 79,510\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.add(dense1) # 은닉층 \n",
    "model.add(dense2) # 출력층 \n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7d4ee02",
   "metadata": {},
   "source": [
    "- 모델 요약 정보 \n",
    "    - 모델에 포함된 층들이 순서대로 나열 \n",
    "        - 첫 은닉층부터 출력층까지 \n",
    "    - 층마다 이름, 클래스, 출력 크기, 파라미터 개수 나옴 \n",
    "        - 이름 \n",
    "            - 층을 만들 때 name 매개변수로 지정 가능 \n",
    "            - 지정하지 않으면 기본값 \"dense\"\n",
    "    - Output Shape \n",
    "        - 출력 크기 \n",
    "        - (None, 100) \n",
    "            - 첫 번째 차원은 샘플의 개수 의미\n",
    "            - None: 데이터 개수가 몇개가 들어와도 된다는 의미 (자리만 잡아둔 것)\n",
    "            - 샘플의 개수가 None인 이유는 한번에 몇 개의 이미지씩 사용할 지 알 수 없기 때문에 어떤 배치 크기에도 유연하게 대응할 수 있도록 None으로 설정 \n",
    "                - 케라스에서는 기본적으로 미니배치 경사 하강법 이용 \n",
    "                - batch_size를 설정하지 않으면 기본값 32\n",
    "                - 따라서 input_shape나 output_shape의 첫번째 차원을 \"배치 차원\"이라고도 부름.\n",
    "            - 두 번째 차원은 출력 개수 \n",
    "                - 100개의 유닛에서 결과가 나오기 때문에 출력 개수가 100 \n",
    "                - 즉, 각 이미지마다 784개의 픽셀값이 은닉층을 통과하면서 100개의 특성으로 압축됨.\n",
    "                \n",
    "    - Param\n",
    "        - 모델 파라미터 개수\n",
    "        - dense 층 \n",
    "            - 784픽셀의 입력값과 100개의 유닛의 모든 조합에 대한 가중치 + 각 유닛의 절편 1개씩 \n",
    "                - 784*100 + 100 = 78500\n",
    "        - dense_1 층 \n",
    "            - 앞 은닉층의 100개 유닛과 10개의 출력층 유닛의 모든 조합에 대한 가중치 + 각 유닛의 절편 1개씩 \n",
    "                - 100*10 + 10 = 1010\n",
    "                \n",
    "    - Non-trainable params\n",
    "        - 훈련되지 않는 파라미터 (고정된 파라미터)\n",
    "        - 경사 하강법으로 훈련되지 않는 파라미터를 가진 층이 있다면 여기에 표시됨."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5891c8fe",
   "metadata": {},
   "source": [
    "## 층을 추가하는 다른 방법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "edabe870",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential([\n",
    "    keras.layers.Dense(100, \n",
    "                       activation='sigmoid', \n",
    "                       input_shape=(784,),\n",
    "                      name='hidden'), # 은닉층 이름 \n",
    "    keras.layers.Dense(10, \n",
    "                       activation='softmax', \n",
    "                       name='output') # 출력층 이름 \n",
    "], name=\"Fashion_MNIST_model\") # 모델 이름 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e774d627",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Fashion_MNIST_model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " hidden (Dense)              (None, 100)               78500     \n",
      "                                                                 \n",
      " output (Dense)              (None, 10)                1010      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 79,510\n",
      "Trainable params: 79,510\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68e32c56",
   "metadata": {},
   "source": [
    "- 여러 모델과 많은 층을 사용할 때 구분을 위해 name 매개변수 사용 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "548086ce",
   "metadata": {},
   "source": [
    "## 모델 훈련 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4aed80df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.5686 - accuracy: 0.8060\n",
      "Epoch 2/5\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.4136 - accuracy: 0.8499\n",
      "Epoch 3/5\n",
      "1500/1500 [==============================] - 2s 2ms/step - loss: 0.3775 - accuracy: 0.8629\n",
      "Epoch 4/5\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.3549 - accuracy: 0.8707\n",
      "Epoch 5/5\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.3368 - accuracy: 0.8788\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x29bfb4ef688>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "             metrics=\"accuracy\")\n",
    "model.fit(scaled_train, y_train, epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a416e1bb",
   "metadata": {},
   "source": [
    "- 단층 신경망 : 0.85 / 0.43\n",
    "- 다층 신경망 : 0.87 / 0.33 \n",
    "\n",
    "--> 은닉층 하나 더 집어넣었을 뿐인데 훈련세트에 대한 성능 크게 향상 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d409f885",
   "metadata": {},
   "source": [
    "# 활성화 함수 \n",
    "\n",
    "- 초창기 인공 신경망의 은닉층에서 많이 사용된 활성화 함수는 시그모이드 \n",
    "<img src = \"sigmoid.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36cf8e1f",
   "metadata": {},
   "source": [
    "- 입력값이 아무리 크더라도 0~1 사이의 값으로 출력되어 출력값의 범위가 너무 좁음. \n",
    "    - 경사하강법 수행 시에 기울기가 0에 수렴하는 \"기울기 소실\"(Gradient Vanishing)이 발생 가능 \n",
    "    - 층이 많아지고 모델이 복잡해질수록 그 효과가 누적되어 더욱 학습을 어렵게 만듦. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b13a592",
   "metadata": {},
   "source": [
    "# 렐루 함수 (ReLU)\n",
    "\n",
    "<img src = \"relu.png\">\n",
    "\n",
    "- 입력이 양수일 경우 마치 활성화 함수가 없는 것처럼 그냥 입력을 통과시키고, 음수일 경우에는 0이 됨. \n",
    "- 표현식 : max(0, z)\n",
    "\n",
    "- 효과\n",
    "    - 속도가 엄청 빨라짐. \n",
    "    - 기울기 소실 문제가 해결됨. \n",
    "    \n",
    "- 리키 렐루 (Ricky ReLU)\n",
    "    - 음수 부분을 완전 0으로 만들지 않고 0에 가깝게 표현 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "198407a7",
   "metadata": {},
   "source": [
    "#  Flatten \n",
    "\n",
    "- 지금까지는 패션 MNIST 데이터가 28*28 크기이기 때문에 인공 신경망에 주입하기 전에 reshape을 이용하여 1차원으로 펼침. \n",
    "- 같은 기능을 위해서 케라스에서는 Flatten 층 제공 \n",
    "    - 샘플의 개수 차원을 제외하고 나머지 모든 입력 차원을 일렬로 펼쳐주는 역할 \n",
    "    - 가중치나 절편이 없음 \n",
    "    - 하지만 입력층과 은닉층 사이에 추가하기 때문에 편의상 층이라고 부르지만 신경망의 깊이가 깊어진 것으로 보지 않음. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "832ec60e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten (Flatten)           (None, 784)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 100)               78500     \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 10)                1010      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 79,510\n",
      "Trainable params: 79,510\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = keras.Sequential()\n",
    "# 2차원 데이터를 그대로 입력으로 --> 784\n",
    "model.add(keras.layers.Flatten(input_shape=(28,28)))\n",
    "model.add(keras.layers.Dense(100, activation='relu'))\n",
    "model.add(keras.layers.Dense(10, activation='softmax'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "239d0a01",
   "metadata": {},
   "source": [
    "- Flatten 층의 파라미터 개수는 0\n",
    "- Flatten 층 추가하면 입력값의 차원을 짐작할 수 있는 것이 장점 \n",
    "    - 784개의 입력이 첫 번째 은닉층에 전달된다는 것이 명확하게 드러남. \n",
    "- 입력 데이터에 대한 전처리 과정을 가능한한 모델에 포함시키는 것이 케라스 API의 철학 중 하나"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4818003",
   "metadata": {},
   "source": [
    "# 새 모델을 위한 데이터 다시 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c91c0210",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.5344 - accuracy: 0.8142\n",
      "Epoch 2/5\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.3982 - accuracy: 0.8582\n",
      "Epoch 3/5\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.3622 - accuracy: 0.8703\n",
      "Epoch 4/5\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.3387 - accuracy: 0.8781\n",
      "Epoch 5/5\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.3228 - accuracy: 0.8851\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x29bfb9c1d08>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(x_train, y_train), (x_test, y_test) = keras.datasets.fashion_mnist.load_data()\n",
    "scaled_train = x_train / 255.0\n",
    "scaled_train, scaled_val, y_train, y_val = train_test_split(scaled_train,\n",
    "                                                           y_train,\n",
    "                                                           test_size=0.2,\n",
    "                                                           random_state=4)\n",
    "\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", metrics=\"accuracy\")\n",
    "model.fit(scaled_train, y_train, epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64f1ff1f",
   "metadata": {},
   "source": [
    "- relu 활성화함수, flatten층 추가한 결과 더 좋아진 모습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "427dc83c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "375/375 [==============================] - 1s 1ms/step - loss: 0.3511 - accuracy: 0.8741\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.35111477971076965, 0.8740833401679993]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(scaled_val, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1acc6be",
   "metadata": {},
   "source": [
    "# 딥러닝의 하이퍼파라미터 \n",
    "\n",
    "- 하이퍼파라미터 : 모델이 학습하지 않아서 사람이 지정해줘야하는 파라미터 \n",
    "- 인공 신경망에서 하이퍼파라미터의 종류\n",
    "     - 은닉층의 개수 \n",
    "     - 은닉층의 유닛 개수 \n",
    "     - 활성화 함수 \n",
    "     - 층의 종류 : 밀집층 / CNN / RNN \n",
    "     - 미니배치 크기 (batch_size)\n",
    "     - 반복 횟수 (epochs)\n",
    "     - 옵티마이저 (optimizer)\n",
    "     - 옵티마이저의 학습률 (learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94e3e22f",
   "metadata": {},
   "source": [
    "# 옵티마이저 \n",
    "\n",
    "- 케라스에서는 기본적으로 경사 하강법 알고리즘(RMSprop) 사용 \n",
    "- 이 외에도 다양한 경사 하강법 알고리즘을 제공하고, 이를 \"옵티마이저\"라고 부름 \n",
    "\n",
    "- 손실함수가 낸 손실을 얼마만큼 어떻게 수정해나갈거냐 하는 역할 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "00aebbd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sgd 옵티마이저를 사용하려면\n",
    "# 방법 1 (정석)\n",
    "sgd = keras.optimizers.SGD()\n",
    "model.compile(optimizer=sgd, \n",
    "              loss=\"sparse_categorical_crossentropy\",\n",
    "              metrics=\"accuracy\")\n",
    "\n",
    "# 방법 2 (위와 동일한 코드)\n",
    "# 원래는 위의 코드처럼 \n",
    "# 각각의 클래스 객체를 만들어서 사용하는 것이 정석 \n",
    "# 번거로움을 피하기 위해 \"sgd\"라고 입력하면 자동으로 \n",
    "# SGD 객체를 생성해줌 \n",
    "model.compile(optimizer=\"sgd\",\n",
    "             loss=\"sparse_categorical_crossentropy\",\n",
    "             metrics=\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "189c0f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 옵티마이저의 학습률을 조절하고 싶다면 \n",
    "sgd = keras.optimizers.SGD(learning_rate=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f25b80f",
   "metadata": {},
   "source": [
    "## 옵티마이저 종류 \n",
    "\n",
    "<img src = \"optimizer.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61811383",
   "metadata": {},
   "source": [
    "- Momentum \n",
    "    - SGD 클래스에서 momentum 기본값은 0 \n",
    "    - 가던 방향으로 계속 가려는 힘 \n",
    "    - momentum을 0보다 큰 값으로 지정하면 모멘텀 최적화(momentum optimization)을 사용 \n",
    "    - 일반적으로 momentum 매개변수는 0.9 이상을 지정 \n",
    "    \n",
    "- NAG (Nesterov Accelerated Gradient; 네스테로프)\n",
    "    - SGD 클래스의 nesterov 매개변수를 기본값 False에서 True로 바꾸면 네스테로프 모멘텀 최적화를 사용 가능 \n",
    "    - 대부분의 경우 네스테로프 모멘텀 최적화가 기본 경사 하강법보다는 더 나은 성능을 제공 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8953dc9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NAG 설정 \n",
    "nag = keras.optimizers.SGD(momentum=0.9, nesterov=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ab912b",
   "metadata": {},
   "source": [
    "- 적응적 학습률 (adaptive learning rate)\n",
    "    - 모델이 최적점에 가까이 갈수록 학습률 낮춤 \n",
    "        - 안정적으로 최적점에 수렴할 가능성 높음 \n",
    "    - 적응적 학습률을 사용하는 대표적인 옵티마이저 \n",
    "        - Adagrad, RMSProp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "611014d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adagrad\n",
    "adagrad = keras.optimizers.Adagrad()\n",
    "model.compile(optimizer=adagrad,\n",
    "             loss=\"sparse_categorical_crossentropy\",\n",
    "             metrics=\"accuracy\")\n",
    "\n",
    "# RMSProp\n",
    "rmsprop = keras.optimizers.RMSprop()\n",
    "model.compile(optimizer=rmsprop,\n",
    "             loss=\"sparse_categorical_crossentropy\",\n",
    "             metrics=\"accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48d1883b",
   "metadata": {},
   "source": [
    "- 위의 \"모멘텀 최적화\"와 \"적응적 학습률\"을 접목한 것이 Adam\n",
    "    - 기본적으로 가장 많이 쓰이는 옵티마이저 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e3533930",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.5265 - accuracy: 0.8144\n",
      "Epoch 2/5\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.3982 - accuracy: 0.8576\n",
      "Epoch 3/5\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.3581 - accuracy: 0.8699\n",
      "Epoch 4/5\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.3319 - accuracy: 0.8782\n",
      "Epoch 5/5\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.3114 - accuracy: 0.8856\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x29bfb862f48>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Flatten(input_shape=(28,28)))\n",
    "model.add(keras.layers.Dense(100, activation='relu'))\n",
    "model.add(keras.layers.Dense(10, activation='softmax'))\n",
    "\n",
    "# Adam \n",
    "model.compile(optimizer='adam', \n",
    "             loss=\"sparse_categorical_crossentropy\",\n",
    "             metrics=\"accuracy\")\n",
    "\n",
    "model.fit(scaled_train, y_train, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "19652df5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "375/375 [==============================] - 1s 1ms/step - loss: 0.3319 - accuracy: 0.8767\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.33192798495292664, 0.8766666650772095]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(scaled_val, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d34525",
   "metadata": {},
   "source": [
    "- 아담 옵티마이저를 쓴 결과 RMSprop(디폴트)보다 아주 살짝 더 성능 좋아짐."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
